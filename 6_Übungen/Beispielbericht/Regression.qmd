---
title: "Regression"
author: "Dirk Ostwald"
date: today
bibliography: referenzen.bib
lang: de
format: 
  pdf:
    include-in-header:
      text: |
       \usepackage[font=small,format=plain,labelfont=bf, labelsep=period,justification=justified,singlelinecheck=false]{caption}
       \setcounter{section}{1}
---

Fundamentales Ziel von Regressionsanalysen ist es, Beziehungen zwischen
unabhängigen und abhängigen Variablen zu modellieren. Ein zentrales Thema dabei
ist die Anpassung von Funktionen an beobachtete Datensätze. Mit dem 
Begriff der *Ausgleichsgerade* im Rahmen der *Methode der kleinsten Quadrate* und
dem Begriff der *einfachen linearen Regression* wollen wir uns in diesem 
Abschnitt diesen zentralen Themen der probabilistischen Datenmodellierung schrittweise 
nähern. Dabei unterscheiden sich die Konzepte von Ausgleichsgerade und einfacher
lineare Regression in einem zentralem Aspekt: bei der Ausgleichsgerade werden
unabhängige und abhängige Variable nicht als Zufallsvariablen modelliert, im 
Rahmen der einfachen linearen Regression nimmt die abhängige Variable dann 
die Form einer Zufallsvariablen an. Im Kontext der Korrelation schließlich werden
sowohl abhängige als auch unabhängige Variable als Zufallsvariablen modelliert.

Um die Konzepte dieses Abschnittes zu verdeutlichen, betrachten wir einen 
Beispieldatensatz in dem die Anzahl an Psychotherapiestunden als unabhängige 
Variable $x$ der Symptomreduktion einer Gruppe von $n = 20$ Patient:innen als 
abhängige Variable $y$ gegenüber gestellt wird (@fig-beispieldatensatz ). 
Die visuelle Inspektion dieses Datensatzes legt nahe, dass ein Mehr an Therapiestunden 
ein Mehr an Symptomreduktion impliziert. Ziel der Methode der kleinsten Quadrate und der 
einfachen linearen Regression ist es, diesen intuitiven funktionalen Zusammenhang
zwischen unabhängiger und abhängiger Variable auf eine quantitative Basis zu stellen.

```{r, echo = F}
library(MASS)                                         # Normalverteilungen
set.seed(0)                                           # Ergebnisreproduzierbarkeit
n           = 20                                      # Anzahl Datenpunkte
p           = 3                                       # Anzahl Regressionskoeffizienten
x           = seq(1,n,len = n)                        # Kontrollvariable
X           = matrix(c(rep(1,n), x, x^2), ncol = 3)   # Designmatrix
beta        = matrix(c(.5,.2,.06), ncol = 1)          # Wahre, unbekannte, Regressionskoeffizientenwerte
mu          = X %*% beta                              # Erwartungswertparameter
sigsqr      = 10                                      # Varianzparameter
Sigma       = sigsqr*diag(n)                          # Kovarianzmatrixparameter
y           = as.matrix(mvrnorm(1,mu,Sigma))          # Datengeneration
D           = data.frame(y_i = y, x_i = x)            # Dataframe

# Datensicherung
fname       = file.path("./Daten/Regression.csv")
write.csv(D, file = fname, row.names = FALSE)
```

 
```{r, echo = F, eval = F}
library(latex2exp())
pdf(
file        = file.path("./Abbildungen/reg_beispieldatensatz.pdf"),
width       = 5,
height      = 5)
par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1)
plot(
x,
y,
pch        = 16,
xlab       = "Anzahl Therapiestunden (x)",
ylab       = "Symptomreduktion (y)",
xlim       = c(0,21),
ylim       = c(-10, 40))
legend(
"topleft",
TeX("$(x_i,y_i)$"),
lty         = 0,
pch         = 16,
col         = "black",
bty         = "n",
cex         = 1,
x.intersp   = 1)
dev.off()
```

![Beispieldatensatz](./Abbildungen/reg_beispieldatensatz.pdf){#fig-beispieldatensatz fig-align="center" width=50%}

## Methode der kleinsten Quadrate {#sec-kq-methode}

Wir definieren zunächst den Begriff der *Ausgleichsgerade*.

::: {#def-ausgleichsgerade}
## Ausgleichsgerade
Für $\beta := (\beta_0,\beta_1)^T \in \mathbb{R}^2$ heißt die linear-affine Funktion
\begin{equation}
f_\beta : \mathbb{R} \to \mathbb{R}, x \mapsto f_\beta(x) := \beta_0 + \beta_1 x,
\end{equation}
für die für einen Datensatz  $\{(x_1,y_1),...,(x_n,y_n)\} \subset \mathbb{R}^2$ die Funktion
\begin{equation}
q : \mathbb{R}^2 \to \mathbb{R}_{\ge 0}, \beta \mapsto q(\beta)
:= \sum_{i=1}^n (y_i-f_\beta(x_i))^2
 = \sum_{i=1}^n (y_i- (\beta_0 + \beta_1x_i))^2
\end{equation}
der quadrierten vertikalen Abweichungen der $y_i$ von den Funktionswerten $f_{\beta}(x_i)$
ihr Minimum annimt, *Ausgleichsgerade für den Datensatz $\{(x_1,y_1),...,(x_n,y_n)\}$*.
:::

Bei der Ausgleichsgerade handelt es sich also um eine *linear-affine Funktion*
der Form
\begin{equation}
f_\beta : \mathbb{R} \to \mathbb{R}, x \mapsto f_\beta(x) := \beta_0 + \beta_1 x.
\end{equation}
@fig-linear-affine-funktionen zeigt drei durch jeweils andere Werte 
von $\beta_0$ und $\beta_1$ parameterisierte  linear-affine Funktionen zusammen 
mit der Wertemenge des Beispieldatensatzes. 

Wie bei allen linear-affinen Funktionen entspricht bei $f_\beta$ der Wert von 
$\beta_0$ dem Wert, den $f_\beta$ für $x = 0$ annimmt,
\begin{equation}
f_\beta(0) = \beta_0 + \beta_1\cdot0 = \beta_0
\end{equation}
und damit graphisch dem Schnittpunkt des Funktionsgraphen mit der $y$-Achse. 
Da $\beta_0$ damit dem Versatz (engl. *offset*) des Funktionsgraphen von $y = 0$ an 
der Stelle $x = 0$  entspricht, nennt man $\beta_0$ auch häufig den *Offsetparameter*. 
Analog entspricht wie bei allen linear-affinen Funktionen der Wert von $\beta_1$ 
dem Wert der Funktionswertdifferenz pro Argumenteinheitsdifferenz. Beispielsweise 
gilt etwa für $\beta_0 = 5$ und $\beta_1 = 0.5$, dass
\begin{align}
\begin{split}
f_\beta(2) - f_\beta(1) & =  (5 + 0.5\cdot2)  - (5 + 0.5\cdot1) = 1 - 0.5 = 0.5 \\
f_\beta(9) - f_\beta(8) & =  (5 + 0.5\cdot 9) - (5 + 0.5\cdot8) = 9.5 - 8 = 0.5
\end{split}
\end{align}
Für eine Argumentdifferenz von $1$ ergibt sich also eine Funktionswertdifferenz
von $0.5$. $\beta_1$ enkodiert also die Stärke der Änderung der Funktionswerte 
pro Argumentseinheitsdifferenz und damit die Steigung (engl. *slope*) des Graphen
der linear-affinen Funktion. Entsprechend wird $\beta_1$ *Steigungsparameter*
oder *Slopeparameter* genannt.


```{r, echo = F, eval = F}
library(latex2exp)
pdf(
file        = file.path("./Abbildungen/reg_linear_affine_funktionen.pdf"),
width       = 12,
height      = 4)

# Ausgleichs- und weitere Geraden
X           = matrix(c(rep(1,n), x), ncol = 2)               # Designmatrix
beta_set    = matrix(c(5,.5, -20,3, -6.2,1.7), nrow = 2)     # Weitere Geraden

# Visualisierung
lab         = c(TeX("$\\beta_0 =   5.0, \\beta_1 = 0.5$"),  # Labels
                TeX("$\\beta_0 = -20.0, \\beta_1 = 3.0$"),
                TeX("$\\beta_0 =  -6.2, \\beta_1 = 1.7$"))
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)
for(i in 1:3){
  plot(
  x,
  y,
  pch         = 16,
  xlab        = "Anzahl Therapiestunden (x)",
  ylab        = "Symptomreduktion (y)",
  xlim        = c(0,21),
  ylim        = c(-10, 40),
  main        = lab[i])
  abline(coef = beta_set[,i], lty = 1, col = "black")
}
dev.off()
```

![Linear-affine Funktionen mit unterschiedlichen Parameterwerten vor dem Hintegrund des Beispieldatensatzes](./Abbildungen/reg_linear_affine_funktionen.pdf){#fig-linear-affine-funktionen fig-align="center" width=100%}

Nach Definition ist die Ausgleichsgerade nun allerdings nicht eine beliebige
linear-affine Funktion der Form $f_\beta$, sondern eben jene, die für einen
gegebenen Datensatze $\{(x_1,y_1),...,(x_n,y_n)\}$ die Summe der quadrierten 
vertikalen Abweichnungen
\begin{equation}
q(\beta) := \sum_{i=1}^n (y_i- (\beta_0 + \beta_1x_i))^2
\end{equation}
minimiert. Für eine fest vorgegebenen Datensatz von $(x_i,y_i)$ Paaren ist der Wert
dieser Summe abhängig von den Werten von $\beta_0$ und $\beta_1$ und kann deshalb
durch Wahl geeigneter Werte von $\beta_0$ und $\beta_1$ minimiert werden. Da hierbei
eine Summe von quadrierten Abweichungen zwischen Datenpunkten und Werten
der Ausgleichsgerade minimiert wird, spricht man auch oft etwas ungenau von 
der *Methode der kleinsten Quadrate* (engl. *method of least squares*). 
@fig-vertikale-Abweichungen zeigt die vertikalen Abweichungen zwischen
$y_i$ und $\beta_0 + \beta_1x_i$ für $i = 1,...,n$ des Beispieldatensatzes als 
orange Linien sowie die Summe ihrer Quadrate $q(\beta)$ im Titel. Für die 
Parameterwerte $\beta_0 = -6.2$ und $\beta_1 = 1.7$ (vgl. @fig-linear-affine-funktionen) 
nimmt diese Summe ihren kleinsten Wert an. 

![Vertikale Abweichungen und Quadratsummen bei unterschiedlichen Parameterwerten](./Abbildungen/reg_vertikale_Abweichungen.pdf){#fig-vertikale-Abweichungen fig-align="center" width=100%} 

```{r, echo = F, eval = F}
library(latex2exp)
pdf(
file        = file.path("./Abbildungen/reg_vertikale_Abweichungen.pdf"),
width       = 12,
height      = 4)

# q Funktionswerte
q1          = t(y - X %*% beta_set[,1]) %*% (y - X %*% beta_set[,1])
q2          = t(y - X %*% beta_set[,2]) %*% (y - X %*% beta_set[,2])
q3          = t(y - X %*% beta_set[,3]) %*% (y - X %*% beta_set[,3])

# Visualisierung
par(
family      = "sans",
mfcol       = c(1,3),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)
lab         = c(TeX("$q(\\beta) = 1159$"),
                TeX("$q(\\beta) = 1451$"),
                TeX("$q(\\beta) = 250$"))

for(i in 1:3){
  plot(
  x,
  y,
  pch         = 16,
  xlab        = "Anzahl Therapiestunden (x)",
  ylab        = "Symptomreduktion (y)",
  xlim        = c(0,21),
  ylim        = c(-10, 40),
  main        = lab[i])
  abline(coef = beta_set[,i], lty = 1, col = "black")
  arrows(
  x0        = x,
  y0        = y,
  x1        = x,
  y1        = X %*% beta_set[,i],
  length    = 0,
  col       = "orange")
}
dev.off()
```


Konkrete Formeln zur Bestimmung der Parameterwerte der Ausgleichsgerade stellt @thm-ausgleichsgerade bereit.

::: {#thm-ausgleichsgerade}
## Ausgleichsgerade
Für einen Datensatz $\{(x_1,y_1),...,(x_n,y_n)\}\subset\mathbb{R}^2$ hat die Ausgleichsgerade die Form
\begin{equation}
f_\beta : \mathbb{R} \to \mathbb{R}, x \mapsto f_\beta(x) := \hat{\beta}_0 + \hat{\beta}_1 x,
\end{equation}
wobei mit der Stichprobenkovarianz $c_{xy}$ der $(x_i,y_i)$-Werte, der
Stichprobenvarianz $s_x^2$ der $x_i$-Werte und den Stichprobenmitteln $\bar{x}$
und $\bar{y}$ der $x_i$- und $y_i$-Werte, respektive, gilt, dass
\begin{equation}
\hat{\beta}_1 = \frac{c_{xy}}{s_x^2} \mbox{ und } \hat{\beta}_0 = \bar{y} - \hat{\beta}_1\bar{x}.
\end{equation}
:::

:::{.proof}

:::

@thm-ausgleichsgerade besagt, dass die Parameterwerte, die für einen gegebenen
Datensatz $\{(x_1,y_1),...,(x_n,y_n)\}$ die Summe der quadrierten vertikalen Abweichungen
für eine linear-affine Funktion minimieren mithilfe der Stichprobenmittel
der $x_i$- und $y_i$-Werte, der Stichprobenvarianz der $x_i$-Werte und der Stichprobenkovarianz
der $x_i$- und $y_i$-Werte berechnet werden können. Die Terminologie orientiert
sich hier an den Begrifflichkeiten der deskriptiven Statistik, insbesondere werden
die $x_i$-Werte häufig *nicht* als Realisationen von Zufallsvariablen verstanden,
der Begriff der Stichprobe wird jedoch trotzdem verwendet. Aus der 
Anwendungsperspektive können nach @thm-ausgleichsgerade die Parameter der 
Ausgleichsgerade also mithilfe der bekannten Funktionen für die Auswertung 
deskriptiver Statistiken bestimmt werden. Folgender **R** Code demonstriert dies.

\tiny
```{r}
# Einlesen des Beispieldatensatzes
fname       = file.path("./Daten/Regression.csv")
D           = read.table(fname, sep = ",", header = TRUE)

# Stichprobenstatistiken
x_bar       = mean(D$x_i)               # Stichprobenmittel der x_i-Werte
y_bar       = mean(D$y_i)               # Stichprobenmittel der y_i-Werte
s2x         = var(D$x_i)                # Stichprobenvarianz der  x_i-Werte
cxy         = cov(D$x_i, D$y_i)         # Stichprobenkovarianz der (x_i,y_i)-Werte

# Ausgleichsgeradenparameter
beta_1_hat  = cxy/s2x                   # \hat{\beta}_1, Steigungsparameter
beta_0_hat  = y_bar - beta_1_hat*x_bar  # \hat{\beta}_0, Offset Parameter

# Ausgabe
cat("beta_0_hat:", beta_0_hat,
    "\nbeta_1_hat:", beta_1_hat)
```
\normalsize

Eine typische Visualisierung der Ausgleichsgerade eines Datensatzes wie in 
@fig-ausgleichsgerade implementiert folgender **R** Code.

\tiny
```{r, eval = F}
# Visualisierung der Datenwerte als Punktwolke
plot(
D$x_i,
D$y_i,
pch         = 16,
xlab        = "Anzahl Therapiestunden (x)",
ylab        = "Symptomreduktion (y)",
xlim        = c(0,21),
ylim        = c(-10, 40),
main        = TeX("$\\hat{\\beta}_0 =  -6.19, \\hat{\\beta}_1 = 1.66$"))

# Ausgleichsgerade
abline(
coef        = c(beta_0_hat, beta_1_hat),
lty         = 1,
col         = "black")

# Legende
legend(
"topleft",
c(TeX("$(x_i,y_i)$"), TeX("$f(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x$")),
lty        = c(0,1),
pch        = c(16, NA),
bty        = "n")
```
\normalsize

```{r, echo = F, eval = F}
pdf(
file        = file.path("./Abbildungen/reg_ausgleichsgerade.pdf"),
width       = 5,
height      = 5)

par(
family      = "sans",
mfcol       = c(1,1),
pty         = "s",
bty         = "l",
lwd         = 1,
las         = 1,
mgp         = c(2,1,0),
xaxs        = "i",
yaxs        = "i",
font.main   = 1,
cex         = 1,
cex.main    = 1.2)

# Datenwerte
plot(
D$x_i,
D$y_i,
pch         = 16,
xlab        = "Anzahl Therapiestunden (x)",
ylab        = "Symptomreduktion (y)",
xlim        = c(0,21),
ylim        = c(-10, 40),
main        = TeX("$\\hat{\\beta}_0 =  -6.19, \\hat{\\beta}_1 = 1.66$"))

# Ausgleichsgerade
abline(
coef        = c(beta_0_hat, beta_1_hat),
lty         = 1,
col         = "black")

# Legende
legend(
"topleft",
c(TeX("$(x_i,y_i)$"), TeX("$f(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x$")),
lty = c(0,1),
pch = c(16, NA),
bty = "n")

dev.off()
```
![Ausgleichsgerade für den Beispieldatensatz](./Abbildungen/reg_ausgleichsgerade.pdf){#fig-ausgleichsgerade fig-align="center" width=50%}

## Literaturhinweise 

Die Idee der Minimierung einer Summe von quadrierten Abweichungen bei der Anpassung
einer Polynomfunktion an beobachtete Werte geht auf die Arbeiten von @legendre1805 
und @gauss1809 im Kontext der Bestimmung von Planetenbahnen zurück. Eine historische
Einordnung dazu gibt @stigler1981. Der Begriff der Regression geht zurück 
auf @galton1886. @stigler1986 gibt dazu einen ausführlichen historischen Überblick.  

## Referenzen